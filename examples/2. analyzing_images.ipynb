{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing images\n",
    "This notebook will build on what we did in the training notebook. First, let us consider the following experiment: We have a group of TRAP2 mice, and we are interested in studying whether the basolateral amygdala (BLA) is involved in two different behavioral tasks. To do so, we induced the expression of tdTomato fluorescent protein in TRAP2 mice by injecting 4OH-tamoxifen during the first behavioral task, labeling the cells activated during this period. After a week of expression, we performed the second different behavioral task, perfused the mice, and performed immunological labeling of cFos protein expression. We took photos of the BLA with a fluorescent microscope, segmented the BLA region, and saved the TIFF images in the folders `tdt` and `cfos`.\n",
    "\n",
    "\n",
    "We will start by importing the necessary packages for our analysis, including the functions from `cellradar`. Ensure you have correctly installed `cellradar` in your Conda environment following the instructions in [README.md](../README.md) and that you are running this notebook with this environment (top right corner of the notebook if you are using VSCode)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-15 10:47:00.816928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from cellradar.main import analyze\n",
    "from cellradar.predicting import colocalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing images with a trained model\n",
    "In this first example, we will use a pre-trained model to predict tdTomato-positive cells and want to use it to identify positive cells in a new set of images. We will first load the model and establish the path where we store the images to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "base.py (348): Trying to unpickle estimator StandardScaler from version 1.2.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "base.py (348): Trying to unpickle estimator PCA from version 1.2.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "base.py (348): Trying to unpickle estimator SVC from version 1.2.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "base.py (348): Trying to unpickle estimator Pipeline from version 1.2.0 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    }
   ],
   "source": [
    "with open(\"./sample_data/tdt_model/best_model_svm.pkl\", \"rb\") as file:\n",
    "    best_model = pkl.load(file)\n",
    "tdt_folder = Path(\"./sample_data/tdt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `analyze` function takes three arguments: `image_folder`, `cmap`, and `best_model`. To obtain more information, we can read the docstring of the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(analyze)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will run the function `analyze` using the defined path to the images and the uploaded machine-learning model. The selected `cmap` is \"Reds\" as the tdTomato protein glows in red under the microscope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model '2D_versatile_fluo' for 'StarDist2D'.\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.479071, nms_thresh=0.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting images: 100%|██████████| 5/5 [00:05<00:00,  1.16s/image]\n",
      "Applying prediction model:   0%|          | 0/5 [00:56<?, ?image/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtdt_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReds\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/cellradar/cellradar/main.py:67\u001b[0m, in \u001b[0;36manalyze\u001b[0;34m(image_folder, cmap, best_model)\u001b[0m\n\u001b[1;32m     64\u001b[0m export_rois(project_folder, rois)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Apply the prediction model to the layers and ROIs\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m \u001b[43miterate_predicting\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/github/cellradar/cellradar/predicting.py:290\u001b[0m, in \u001b[0;36miterate_predicting\u001b[0;34m(layers, rois, cmap, project_folder, best_model)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m tqdm(rois\u001b[38;5;241m.\u001b[39mkeys(), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplying prediction model\u001b[39m\u001b[38;5;124m\"\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;66;03m# Analyze ROIs and get the filtered list\u001b[39;00m\n\u001b[0;32m--> 290\u001b[0m         keeped \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrois\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcmap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m         \u001b[38;5;66;03m# Count the number of positive ROIs (cells)\u001b[39;00m\n\u001b[1;32m    293\u001b[0m         final_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(keeped)\n",
      "File \u001b[0;32m~/Documents/github/cellradar/cellradar/predicting.py:194\u001b[0m, in \u001b[0;36manalyze_image\u001b[0;34m(tag, layers, rois, cmap, project_folder, best_model)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;66;03m# Process ROIs\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m best_model:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# Extract features and classify ROIs using the model\u001b[39;00m\n\u001b[0;32m--> 194\u001b[0m     roi_props \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_stats_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroi_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     results \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    196\u001b[0m         roi_name: analyze_roi(roi_name, roi_stats, best_model)\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m roi_name, roi_stats \u001b[38;5;129;01min\u001b[39;00m roi_props\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    198\u001b[0m     }\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# Keep ROIs that the model classifies as positive\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/github/cellradar/cellradar/utils.py:356\u001b[0m, in \u001b[0;36mcreate_stats_dict\u001b[0;34m(roi_dict, layer)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# Extract statistics for each ROI\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m roi_name, roi_info \u001b[38;5;129;01min\u001b[39;00m roi_dict\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 356\u001b[0m     cell_mask, stats_dict \u001b[38;5;241m=\u001b[39m \u001b[43mextract_roi_stats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroi_info\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    357\u001b[0m     roi_props[roi_name] \u001b[38;5;241m=\u001b[39m stats_dict\n\u001b[1;32m    358\u001b[0m     cell_masks\u001b[38;5;241m.\u001b[39mappend(cell_mask)\n",
      "File \u001b[0;32m~/Documents/github/cellradar/cellradar/utils.py:282\u001b[0m, in \u001b[0;36mextract_roi_stats\u001b[0;34m(layer, roi_info)\u001b[0m\n\u001b[1;32m    279\u001b[0m prop \u001b[38;5;241m=\u001b[39m regionprops(label(cell_mask_cropped))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    281\u001b[0m \u001b[38;5;66;03m# Compute Histogram of Oriented Gradients (HOG) features\u001b[39;00m\n\u001b[0;32m--> 282\u001b[0m h_values, hog_image \u001b[38;5;241m=\u001b[39m \u001b[43mhog\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_cropped_masked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43morientations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixels_per_cell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcells_per_block\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mL2-Hys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Flatten the HOG features if needed (though skimage.hog already returns a flat array if feature_vector=True)\u001b[39;00m\n\u001b[1;32m    293\u001b[0m hog_descriptor_values \u001b[38;5;241m=\u001b[39m h_values\u001b[38;5;241m.\u001b[39mflatten()\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cellradar/lib/python3.8/site-packages/skimage/_shared/utils.py:316\u001b[0m, in \u001b[0;36mchannel_as_last_axis.__call__.<locals>.fixed_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m channel_axis \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel_axis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m channel_axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;66;03m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m#       supporting a tuple of channel axes. Right now, only an\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m#       integer or a single-element tuple is supported, though.\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(channel_axis):\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cellradar/lib/python3.8/site-packages/skimage/feature/_hog.py:256\u001b[0m, in \u001b[0;36mhog\u001b[0;34m(image, orientations, pixels_per_cell, cells_per_block, block_norm, visualize, transform_sqrt, feature_vector, channel_axis)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m o, dr, dc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(orientations_arr, dr_arr, dc_arr):\n\u001b[1;32m    254\u001b[0m                 centre \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m([r \u001b[38;5;241m*\u001b[39m c_row \u001b[38;5;241m+\u001b[39m c_row \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m                                 c \u001b[38;5;241m*\u001b[39m c_col \u001b[38;5;241m+\u001b[39m c_col \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m--> 256\u001b[0m                 rr, cc \u001b[38;5;241m=\u001b[39m \u001b[43mdraw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m                                   \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcentre\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m                 hog_image[rr, cc] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m orientation_histogram[r, c, o]\n\u001b[1;32m    262\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;03mThe fourth stage computes normalization, which takes local groups of\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;03mcells and contrast normalizes their overall responses before passing\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03mGradient (HOG) descriptors.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/cellradar/lib/python3.8/site-packages/skimage/draw/draw.py:392\u001b[0m, in \u001b[0;36mline\u001b[0;34m(r0, c0, r1, c1)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mline\u001b[39m(r0, c0, r1, c1):\n\u001b[1;32m    354\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Generate line pixel coordinates.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \n\u001b[1;32m    356\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8)\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_line\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc1\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "analyze(tdt_folder, 'Reds', best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have just predicted the tdTomato-positive cells in the sample dataset! In the output folder, named `tdt_analysis`, we have the results of the analysis:\n",
    "\n",
    "- `labelled_images`: the rendered images comparing the original image with the identified tdTomato-positive cells overlapped.\n",
    "- `rois_processed`: the identified tdTomato-positive cells after being processed by our machine-learning model.\n",
    "- `rois_raw`: the resulting segmented Regions of interest (ROIs) that will be the input for the machine-learning model. \n",
    "- `counts`: the number of predicted cells for each image.: the number of predicted cells for each image.\n",
    "\n",
    "We can examine the results checking the results table and opening an image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>num_cells</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C2-34454_B2_10X_BLA_002</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C2-34454_B2_10X_BLA_001</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C2-34453_B1_10X_BLA_001</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C2-34453_B1_10X_BLA_002</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C2-34454_B2_10X_BLA_</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 file_name  num_cells\n",
       "0  C2-34454_B2_10X_BLA_002         18\n",
       "1  C2-34454_B2_10X_BLA_001         31\n",
       "2  C2-34453_B1_10X_BLA_001         16\n",
       "3  C2-34453_B1_10X_BLA_002         22\n",
       "4     C2-34454_B2_10X_BLA_         20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"./sample_data/tdt_analysis/counts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"./sample_data/tdt_analysis/labelled_images/C2-34453_B1_10X_BLA_001.png\")\n",
    "plt.imshow(image)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing images without a trained model\n",
    "Sometimes, we do not have a trained machine learning model because there are many cells in the images, so it would be time-consuming, or maybe because we are just exploring the dataset before training an actual model. In these cases, it is possible to perform an analysis without specifying the `best_model` argument on the function `analyze`. In this case, it will just filter out the oversized segmentations. You will notice that the results are only sometimes accurate; therefore, I recommend training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfos_folder = Path(\"./sample_data/cfos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found model '2D_versatile_fluo' for 'StarDist2D'.\n",
      "Loading network weights from 'weights_best.h5'.\n",
      "Loading thresholds from 'thresholds.json'.\n",
      "Using default values: prob_thresh=0.479071, nms_thresh=0.3.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting images:   0%|          | 0/5 [00:00<?, ?image/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9f1a72f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting images:  20%|██        | 1/5 [00:01<00:05,  1.41s/image]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f9f1a72f1f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Segmenting images: 100%|██████████| 5/5 [00:06<00:00,  1.24s/image]\n",
      "Applying prediction model: 100%|██████████| 5/5 [00:04<00:00,  1.04image/s]\n"
     ]
    }
   ],
   "source": [
    "analyze(cfos_folder, 'Greens')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating colocalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predicting import colocalize\n",
    "\n",
    "processed_rois_path_1 = Path('C:/Users/mcanela/Desktop/sample_jose/cfos_analysis/rois_processed')\n",
    "images_path_1 = Path('C:/Users/mcanela/Desktop/sample_jose/cfos')\n",
    "processed_rois_path_2 = Path('C:/Users/mcanela/Desktop/sample_jose/tdt_analysis_with_model/rois_processed')\n",
    "images_path_2 = Path('C:/Users/mcanela/Desktop/sample_jose/tdt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colocalize(processed_rois_path_1, images_path_1, processed_rois_path_2, images_path_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = image_folder.parent / 'features.csv'\n",
    "# with open(path, 'rb') as file:\n",
    "#     best_model = pkl.load(file)\n",
    "# df = pd.read_csv(path, index_col=None)\n",
    "# df = df.drop(columns=['Unnamed: 0'])\n",
    "# X = X_test\n",
    "# y = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"C:/Users/mcanela/Desktop/sample_jose/tdt_analysis_with_model/counts.csv\")\n",
    "# split_df = df[\"file_name\"].str.split(\"_\", expand=True)\n",
    "# df[\"id\"] = split_df[0]\n",
    "# grouped_df = df.groupby(\"id\")[\"num_cells\"].mean().reset_index()\n",
    "# grouped_df.to_excel(\"C:/Users/mcanela/Desktop/sample_jose/counts_grouped.xlsx\", index=False)\n",
    "\n",
    "\n",
    "# split_df = df[\"file_name\"].str.split(\"_\", expand=True)\n",
    "# df[[\"group\", \"id\", \"brain\", \"replica\"]] = split_df\n",
    "# grouped_df = df.groupby([\"group\", \"id\", \"brain\"])[\"cells_per_squared_mm\"].mean().reset_index()\n",
    "# grouped_df.columns = [\"group\", \"id\", \"brain\", \"mean_cells_per_squared_mm\"]\n",
    "# grouped_df.to_excel(os.path.join(output_folder, \"results_friendly.xlsx\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
